{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexander-toschev/ai-tools/blob/main/text/text_theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6451dcc1",
      "metadata": {
        "id": "6451dcc1"
      },
      "source": [
        "# üß† –õ–µ–∫—Ü–∏—è: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –≤ Python\n",
        "–¶–µ–ª—å ‚Äî –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –æ—Å–Ω–æ–≤–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –æ—á–∏—Å—Ç–∫—É, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9059fdd2",
      "metadata": {
        "id": "9059fdd2"
      },
      "source": [
        "## üìò –ß—Ç–æ —Ç–∞–∫–æ–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "- –ù–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: –æ—Ç–∑—ã–≤—ã, —á–∞—Ç—ã, –¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
        "- –ü–æ–ª—É-—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: HTML, XML, JSON\n",
        "- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã —Å –∫–æ–ª–æ–Ω–∫–æ–π `text`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b887ac90",
      "metadata": {
        "id": "b887ac90"
      },
      "source": [
        "## üõ† –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ Python –¥–ª—è NLP\n",
        "- `nltk` ‚Äî –±–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —Å—Ç–µ–º–º–∏–Ω–≥)\n",
        "- `spaCy` ‚Äî –±—ã—Å—Ç—Ä—ã–π –∏ –º–æ—â–Ω—ã–π –∞–Ω–∞–ª–∏–∑: POS-—Ç–µ–≥–∏, —Å—É—â–Ω–æ—Å—Ç–∏\n",
        "- `re` ‚Äî —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è\n",
        "- `scikit-learn` ‚Äî –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (TF-IDF)\n",
        "- `transformers` ‚Äî –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ BERT, GPT –∏ –¥—Ä."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49c6dbad",
      "metadata": {
        "id": "49c6dbad"
      },
      "source": [
        "## üîß –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "- –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "- –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, HTML-—Ç–µ–≥–æ–≤\n",
        "- –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
        "- –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏/–∏–ª–∏ —Å—Ç–µ–º–º–∏–Ω–≥"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be3a6b31",
      "metadata": {
        "id": "be3a6b31"
      },
      "source": [
        "## üìö –ß—Ç–æ —Ç–∞–∫–æ–µ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è?\n",
        "**–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è** ‚Äî —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è —Å–ª–æ–≤–∞ –∫ –µ–≥–æ –±–∞–∑–æ–≤–æ–π —Å–ª–æ–≤–∞—Ä–Ω–æ–π —Ñ–æ—Ä–º–µ.\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä—ã:**\n",
        "- `running`, `ran`, `runs` ‚Üí `run`\n",
        "- `was`, `were` ‚Üí `be`\n",
        "- `better` ‚Üí `good`\n",
        "\n",
        "–¶–µ–ª—å ‚Äî –ø—Ä–∏–≤–µ—Å—Ç–∏ –≤—Å–µ —Ñ–æ—Ä–º—ã –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∫ –æ–¥–Ω–æ–π –±–∞–∑–µ, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –∞–Ω–∞–ª–∏–∑.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "932de9ce",
      "metadata": {
        "id": "932de9ce"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"running\"))        # ‚ûú run\n",
        "print(lemmatizer.lemmatize(\"better\", pos='a')) # ‚ûú good\n",
        "print(lemmatizer.lemmatize(\"feet\"))           # ‚ûú foot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e8c8112",
      "metadata": {
        "id": "4e8c8112"
      },
      "source": [
        "### üìä –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è vs –°—Ç–µ–º–º–∏–Ω–≥\n",
        "- **–°—Ç–µ–º–º–∏–Ω–≥** ‚Äî –æ–±—Ä–µ–∑–∞–µ—Ç –æ–∫–æ–Ω—á–∞–Ω–∏—è —Å–ª–æ–≤, –±—ã—Å—Ç—Ä—ã–π, –Ω–æ –≥—Ä—É–±—ã–π.\n",
        "- **–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è** ‚Äî —É—á–∏—Ç—ã–≤–∞–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏–∫—É –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ª–æ–≤–∞—Ä–∏.\n",
        "\n",
        "|     | –°—Ç–µ–º–º–∏–Ω–≥ | –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è |\n",
        "|-----|----------|---------------|\n",
        "| –°–∫–æ—Ä–æ—Å—Ç—å | –ë—ã—Å—Ç—Ä–µ–µ | –ú–µ–¥–ª–µ–Ω–Ω–µ–µ |\n",
        "| –¢–æ—á–Ω–æ—Å—Ç—å | –ù–∏–∂–µ | –í—ã—à–µ |\n",
        "| –û—Å–Ω–æ–≤–∞ | –û–±—Ä–µ–∑–∫–∞ | –°–ª–æ–≤–∞—Ä–∏ + –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c550f55",
      "metadata": {
        "id": "1c550f55"
      },
      "source": [
        "## üî† –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
        "- **Bag of Words**: —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤\n",
        "- **TF-IDF**: —á–∞—Å—Ç–æ—Ç–∞ —Å —É—á–µ—Ç–æ–º –≤–∞–∂–Ω–æ—Å—Ç–∏\n",
        "- **Word2Vec / BERT**: –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77363b16",
      "metadata": {
        "id": "77363b16"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\"this is a text\", \"this is another text\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa6ce2c8",
      "metadata": {
        "id": "aa6ce2c8"
      },
      "source": [
        "## üß† –ü—Ä–∏–º–µ—Ä: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ (20 newsgroups)\n",
        "- –ö–ª–∞—Å—Å—ã: —Å–ø–æ—Ä—Ç, –º–µ–¥–∏—Ü–∏–Ω–∞, –∫–æ–º–ø—å—é—Ç–µ—Ä—ã –∏ —Ç.–¥.\n",
        "- –ú–æ–¥–µ–ª—å: `TfidfVectorizer` + `Naive Bayes`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "138967e1",
      "metadata": {
        "id": "138967e1"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "data = fetch_20newsgroups(subset='train', categories=['sci.med', 'rec.sport.baseball'])\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model.fit(data.data, data.target)\n",
        "print(model.predict([\"The player hit a home run\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c451f0",
      "metadata": {
        "id": "c4c451f0"
      },
      "source": [
        "## üìå –í—ã–≤–æ–¥\n",
        "- –¢–µ–∫—Å—Ç –Ω—É–∂–Ω–æ –æ—á–∏—â–∞—Ç—å –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å\n",
        "- –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ–∫—Ä–∞—Ç–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –∏ –ø–æ–≤—ã—Å–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ\n",
        "- TF-IDF ‚Äî —Ö–æ—Ä–æ—à–∏–π –±–∞–∑–æ–≤—ã–π —Å–ø–æ—Å–æ–± –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\n",
        "- –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (BERT, GPT –∏ –¥—Ä.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163e9569",
      "metadata": {
        "id": "163e9569"
      },
      "source": [
        "## ü§ñ –ß—Ç–æ —Ç–∞–∫–æ–µ BERT\n",
        "**BERT (Bidirectional Encoder Representations from Transformers)** ‚Äî —ç—Ç–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ—Ç Google, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–æ–≤ –≤ –æ–±–µ —Å—Ç–æ—Ä–æ–Ω—ã ‚Äî –∏ —Å–ª–µ–≤–∞, –∏ —Å–ø—Ä–∞–≤–∞.\n",
        "\n",
        "–ù–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–æ `bank` –≤ –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö:\n",
        "- \"He went to the **bank** to deposit money\" ‚Üí –±–∞–Ω–∫\n",
        "- \"She sat by the **bank** of the river\" ‚Üí –±–µ—Ä–µ–≥\n",
        "\n",
        "**BERT** –ø–æ–Ω–∏–º–∞–µ—Ç —Ä–∞–∑–Ω–∏—Ü—É –∏ —Å–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.\n",
        "\n",
        "–û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è:\n",
        "- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
        "- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
        "- –í–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç —Å–∏—Å—Ç–µ–º\n",
        "- –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a488d3ac",
      "metadata": {
        "id": "a488d3ac"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "print(classifier('I really enjoyed this lesson!'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61f98a34",
      "metadata": {
        "id": "61f98a34"
      },
      "source": [
        "## üî¢ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7721e2be",
      "metadata": {
        "id": "7721e2be"
      },
      "source": [
        "### 1. Bag of Words (BoW)\n",
        "–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –≤ –≤–µ–∫—Ç–æ—Ä —á–∞—Å—Ç–æ—Ç —Å–ª–æ–≤.\n",
        "- –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
        "- –ü—Ä–æ—Å—Ç –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä:**\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer()\n",
        "X = vec.fit_transform([\"I love NLP\", \"NLP is fun\"])\n",
        "print(X.toarray())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f61331c6",
      "metadata": {
        "id": "f61331c6"
      },
      "source": [
        "### 2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "–í–µ–∫—Ç–æ—Ä, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏ –µ–≥–æ —Ä–µ–¥–∫–æ—Å—Ç—å –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏.\n",
        "- –°–Ω–∏–∂–∞–µ—Ç –≤–µ—Å —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"the\")\n",
        "- –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä:**\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer()\n",
        "X = vec.fit_transform([\"this is a text\", \"this is another text\"])\n",
        "print(X.toarray())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59eb4165",
      "metadata": {
        "id": "59eb4165"
      },
      "source": [
        "### 3. Word2Vec / GloVe\n",
        "- –ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –∫–∞–∫ –ø–ª–æ—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä (–æ–±—ã—á–Ω–æ –¥–ª–∏–Ω—ã 300)\n",
        "- –ü–æ—Ö–æ–∂–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ‚Üí –ø–æ—Ö–æ–∂–∏–π –≤–µ–∫—Ç–æ—Ä\n",
        "- –¢—Ä–µ–±—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≥–æ—Ç–æ–≤—ã—Ö –≤–µ—Å–æ–≤\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä (gensim Word2Vec):**\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "sentences = [[\"cat\", \"sits\", \"on\", \"the\", \"mat\"]]\n",
        "model = Word2Vec(sentences, vector_size=50, min_count=1)\n",
        "print(model.wv['cat'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7466972a",
      "metadata": {
        "id": "7466972a"
      },
      "source": [
        "### 4. BERT / Sentence-BERT\n",
        "- –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç —Å–ª–æ–≤–∞ –∏–ª–∏ —Ü–µ–ª—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "- –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫, –∫–æ–Ω—Ç–µ–∫—Å—Ç, –≥—Ä–∞–º–º–∞—Ç–∏–∫—É\n",
        "- –£–Ω–∏–≤–µ—Ä—Å–∞–ª–µ–Ω –¥–ª—è –ª—é–±—ã—Ö NLP –∑–∞–¥–∞—á\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä:**\n",
        "```python\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embedding = model.encode(\"Natural Language Processing is cool\")\n",
        "print(embedding.shape)  # ‚ûú (384,)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8bce77c",
      "metadata": {
        "id": "e8bce77c"
      },
      "source": [
        "### üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤\n",
        "| –ú–µ—Ç–æ–¥      | –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫? | –ö–æ–Ω—Ç–µ–∫—Å—Ç? | –¢–∏–ø –≤—ã—Ö–æ–¥–∞ | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å |\n",
        "|------------|--------------------|-----------|------------|-------------|\n",
        "| BoW        | ‚ùå                 | ‚ùå        | –î–æ–∫—É–º–µ–Ω—Ç   | #—Å–ª–æ–≤       |\n",
        "| TF-IDF     | ‚ùå                 | ‚ùå        | –î–æ–∫—É–º–µ–Ω—Ç   | #—Å–ª–æ–≤       |\n",
        "| Word2Vec   | ‚ùå                 | ‚úÖ —á–∞—Å—Ç–∏—á–Ω–æ | –°–ª–æ–≤–æ      | ~300        |\n",
        "| BERT       | ‚úÖ                 | ‚úÖ        | –°–ª–æ–≤–æ/—Ñ—Ä–∞–∑–∞| 768+        |"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}