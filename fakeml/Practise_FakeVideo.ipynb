{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a17ee79-e778-4f7e-8955-7be61e3f6751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting face-recognition\n",
      "  Using cached face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting face-recognition-models>=0.3.0 (from face-recognition)\n",
      "  Using cached face_recognition_models-0.3.0-py2.py3-none-any.whl\n",
      "Collecting Click>=6.0 (from face-recognition)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting dlib>=19.7 (from face-recognition)\n",
      "  Using cached dlib-19.24.8.tar.gz (3.4 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/alex/miniconda3/lib/python3.11/site-packages (from face-recognition) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /home/alex/miniconda3/lib/python3.11/site-packages (from face-recognition) (10.2.0)\n",
      "Using cached face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Building wheels for collected packages: dlib\n",
      "  Building wheel for dlib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dlib: filename=dlib-19.24.8-cp311-cp311-linux_x86_64.whl size=4145126 sha256=00cc317b33713e28040fa71c6548d84f536d656d848dcd560de20dd526f5e297\n",
      "  Stored in directory: /home/alex/.cache/pip/wheels/45/df/3c/d9d5185a237d3c39571ab9a9fac6b984927b826d80dbd1569b\n",
      "Successfully built dlib\n",
      "Installing collected packages: face-recognition-models, dlib, Click, face-recognition\n",
      "Successfully installed Click-8.1.8 dlib-19.24.8 face-recognition-1.3.0 face-recognition-models-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install  face-recognition \n",
    "# python3 faceforensics_download_v4.py -d 'DeepFakeDetection'  data --server EU2 --c c40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf054ac8-a8f4-41a6-9614-a249d29af0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/intern/alex/exception-ffc'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed433578-74b1-4787-a199-4ae63ea92528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:25:47.287288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745508347.302366 2004477 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745508347.307001 2004477 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745508347.319355 2004477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745508347.319373 2004477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745508347.319375 2004477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745508347.319377 2004477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-24 18:25:47.323102: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb24d64-c6ae-4ff5-b8c0-5659815abc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "filtered = gpus[2:5]\n",
    "print (filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057e0d4a-3805-4cf0-9360-8dff6cc6f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_visible_devices(filtered, 'GPU')  # Используем только 2 GPU\n",
    "\n",
    "# (опц.) Устанавливаем память на рост по запросу\n",
    "for gpu in gpus[:2]:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c53bc3-8f91-4b8f-988b-2fa5d9b770ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting real: 100%|███████████████████████████████████████████████████| 50/50 [24:12<00:00, 29.06s/it]\n",
      "Extracting fake: 100%|███████████████████████████████████████████████████| 50/50 [08:18<00:00,  9.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# DeepFake Detection using XceptionNet fine-tuned on DeepFakeDetection dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Настройки ===\n",
    "DATASET_PATH = '/home/intern/alex/exception-ffc/dataset'\n",
    "REAL_DIR = '/home/intern/alex/exception-ffc/data/original_sequences/actors/c40/videos'\n",
    "FAKE_DIR = '/home/intern/alex/exception-ffc/data/manipulated_sequences/DeepFakeDetection/c40/videos'\n",
    "IMG_SIZE = 299\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "\n",
    "os.makedirs(f\"{DATASET_PATH}/real\", exist_ok=True)\n",
    "os.makedirs(f\"{DATASET_PATH}/fake\", exist_ok=True)\n",
    "\n",
    "# === Извлечение лиц из видео ===\n",
    "def extract_faces_from_video(video_path, save_dir, label_prefix, max_faces=10):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    saved = 0\n",
    "    frame_id = 0\n",
    "    while saved < max_faces:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        face_locations = face_recognition.face_locations(frame)\n",
    "        if not face_locations:\n",
    "            continue\n",
    "        top, right, bottom, left = face_locations[0]\n",
    "        face = frame[top:bottom, left:right]\n",
    "        face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
    "        fname = f\"{label_prefix}_{frame_id}.jpg\"\n",
    "        cv2.imwrite(os.path.join(save_dir, fname), face)\n",
    "        saved += 1\n",
    "        frame_id += 1\n",
    "    cap.release()\n",
    "\n",
    "# === Обработка реальных и фейковых видео ===\n",
    "real_videos = [f for f in os.listdir(REAL_DIR) if f.endswith('.mp4')][:50]  # ограничим по 5 видео\n",
    "fake_videos = [f for f in os.listdir(FAKE_DIR) if f.endswith('.mp4')][:50]\n",
    "\n",
    "for fname in tqdm(real_videos, desc='Extracting real'):\n",
    "    extract_faces_from_video(os.path.join(REAL_DIR, fname), f\"{DATASET_PATH}/real\", fname[:-4])\n",
    "\n",
    "for fname in tqdm(fake_videos, desc='Extracting fake'):\n",
    "    extract_faces_from_video(os.path.join(FAKE_DIR, fname), f\"{DATASET_PATH}/fake\", fname[:-4])\n",
    "\n",
    "# === Построение модели ===\n",
    "base_model = Xception(weights=None, include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "base_model.load_weights('/home/intern/alex/exception-ffc/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')  # загрузи вручную через панель файлов\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Загрузка изображений и генерация выборки ===\n",
    "def load_images_from_directory(base_dir, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for fname in os.listdir(base_dir):\n",
    "        fpath = os.path.join(base_dir, fname)\n",
    "        img = cv2.imread(fpath)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        images.append(img.astype('float32') / 255.0)\n",
    "        labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "real_images, real_labels = load_images_from_directory(f\"{DATASET_PATH}/real\", 0)\n",
    "fake_images, fake_labels = load_images_from_directory(f\"{DATASET_PATH}/fake\", 1)\n",
    "\n",
    "X = np.array(real_images + fake_images)\n",
    "y = np.array(real_labels + fake_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52a1007-d968-45e7-8ec9-71ab99007b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:10:44.571426: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 858249600 exceeds 10% of free system memory.\n",
      "2025-04-24 19:10:49.338917: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 858249600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - accuracy: 0.7433 - loss: 0.5770 - val_accuracy: 0.4800 - val_loss: 19.9293\n",
      "Epoch 2/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 120ms/step - accuracy: 0.8630 - loss: 0.3626 - val_accuracy: 0.8100 - val_loss: 1.6712\n",
      "Epoch 3/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 119ms/step - accuracy: 0.9075 - loss: 0.2463 - val_accuracy: 0.5800 - val_loss: 1.2931\n",
      "Epoch 4/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 120ms/step - accuracy: 0.9375 - loss: 0.2077 - val_accuracy: 0.8300 - val_loss: 0.3641\n",
      "Epoch 5/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 119ms/step - accuracy: 0.9613 - loss: 0.1261 - val_accuracy: 0.7400 - val_loss: 0.9598\n",
      "Epoch 6/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 120ms/step - accuracy: 0.9791 - loss: 0.1002 - val_accuracy: 0.9700 - val_loss: 0.2138\n",
      "Epoch 7/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 119ms/step - accuracy: 0.9812 - loss: 0.0976 - val_accuracy: 0.5300 - val_loss: 5.2013\n",
      "Epoch 8/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 120ms/step - accuracy: 0.9561 - loss: 0.1216 - val_accuracy: 0.9500 - val_loss: 0.1232\n",
      "Epoch 9/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 119ms/step - accuracy: 0.9877 - loss: 0.0583 - val_accuracy: 0.9700 - val_loss: 0.0912\n",
      "Epoch 10/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 119ms/step - accuracy: 0.9817 - loss: 0.0573 - val_accuracy: 0.9850 - val_loss: 0.0587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fc2197b1010>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Обучение модели ===\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb1e0b2-fd11-4ea4-96cc-582d7e300b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Модель обучена и сохранена в файл xception_dfdc_trained.h5\n"
     ]
    }
   ],
   "source": [
    "# === Сохранение модели ===\n",
    "model.save('xception_dfdc_trained.keras')\n",
    "print(\"\\nМодель обучена и сохранена в файл xception_dfdc_trained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1c7383c-d337-47c5-9c51-36032013af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Реальное видео — вероятность фейка: 0.34\n",
      "Фейковое видео — вероятность фейка: 1.00\n"
     ]
    }
   ],
   "source": [
    "# DeepFake Detection using XceptionNet (Keras version)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "# === Настройки ===\n",
    "IMG_SIZE = 299\n",
    "MODEL_PATH = 'xception_dfdc_trained.keras'  # Предобученная модель\n",
    "\n",
    "# === Загрузка обученной модели ===\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# === Преобразование для входа ===\n",
    "def preprocess_face(face):\n",
    "    face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
    "    face = face.astype('float32') / 255.0\n",
    "    return np.expand_dims(face, axis=0)\n",
    "\n",
    "# === Извлечение лица ===\n",
    "def extract_face(frame):\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    if not face_locations:\n",
    "        return None\n",
    "    top, right, bottom, left = face_locations[0]\n",
    "    face = frame[top:bottom, left:right]\n",
    "    return face\n",
    "\n",
    "# === Анализ видео ===\n",
    "def analyze_video(video_path, max_frames=15):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    predictions = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        face = extract_face(frame)\n",
    "        if face is None:\n",
    "            continue\n",
    "        input_face = preprocess_face(face)\n",
    "        pred = model.predict(input_face, verbose=0)[0][0]\n",
    "        predictions.append(pred)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    if predictions:\n",
    "        return np.mean(predictions)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# === Пример использования ===\n",
    "video_real = 'data/RealVideo.mp4'  # Замените на путь к своему видео\n",
    "video_fake = 'data/FakeReal.mp4'  # Замените на путь к своему видео\n",
    "\n",
    "real_score = analyze_video(video_real)\n",
    "fake_score = analyze_video(video_fake)\n",
    "\n",
    "print(f\"\\nРеальное видео — вероятность фейка: {real_score:.2f}\")\n",
    "print(f\"Фейковое видео — вероятность фейка: {fake_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aeccce-0963-4187-bd1d-ad87f2c25b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
