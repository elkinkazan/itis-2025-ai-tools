{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elkinkazan/itis-2025-ai-tools/blob/main/Task4_Text_Iliasova.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY  !!!\n",
        "# DO NOT EXECUTE !!!\n",
        "!pip install --upgrade gspread pandas google-auth\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from IPython.display import display\n",
        "import random\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "wIGlJ4MNqejy"
      },
      "id": "wIGlJ4MNqejy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILL THIS\n",
        "student_name = \"ELVIRA ILIASOVA\"\n",
        "group_id = \"11-451\""
      ],
      "metadata": {
        "id": "qdgs81yUqtKK"
      },
      "id": "qdgs81yUqtKK",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY  !!!\n",
        "# DO NOT EXECUTE !!!\n",
        "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1Kfxj2eDFl7xQnXw7Fpb9bwghc65o8xf--VpNxrdWHaY/edit?gid=0#gid=0\"\n",
        "sh = gc.open_by_url(SPREADSHEET_URL)\n",
        "worksheet = sh.sheet1\n",
        "\n",
        "# Ensure header row exists\n",
        "if not worksheet.get_all_values():\n",
        "    worksheet.append_row([\"Student Name\", \"Group\",\"TaskID\", \"Score\"])\n",
        "\n",
        "\n",
        "# MAIN NOTEBOOK GOES HERE\n",
        "task_id = \"Task4_Text\"\n",
        "score = 0\n",
        "max_score = 10"
      ],
      "metadata": {
        "id": "oSBPPoIyqi2v"
      },
      "id": "oSBPPoIyqi2v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d632241f",
      "metadata": {
        "id": "d632241f"
      },
      "source": [
        "# ‚úçÔ∏è –ü—Ä–∞–∫—Ç–∏–∫–∞ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ Python\n",
        "–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –≤—ã –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∑–∞–¥–∞–Ω–∏—è:\n",
        "- –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
        "- –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
        "- TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n",
        "- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
        "- –ê–≤—Ç–æ—Ç–µ—Å—Ç—ã —á–µ—Ä–µ–∑ `unittest`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a0fa0570",
      "metadata": {
        "id": "a0fa0570"
      },
      "outputs": [],
      "source": [
        "# üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "!pip install nltk scikit-learn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a9eb14ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9eb14ed",
        "outputId": "c4086def-7a31-48dc-c3de-35543ea77976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# üì• –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbd7c9c8",
      "metadata": {
        "id": "dbd7c9c8"
      },
      "source": [
        "## üîß –ó–∞–¥–∞–Ω–∏–µ 1: –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "–†–µ–∞–ª–∏–∑—É–π —Ñ—É–Ω–∫—Ü–∏—é `clean_text`, –∫–æ—Ç–æ—Ä–∞—è:\n",
        "- –ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "- –£–¥–∞–ª—è–µ—Ç HTML-—Ç–µ–≥–∏ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
        "- –£–¥–∞–ª—è–µ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "44bc5539",
      "metadata": {
        "id": "44bc5539"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text.lower())  # –£–¥–∞–ª–µ–Ω–∏–µ HTML –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)       # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —Ü–∏—Ñ—Ä\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # YOUR CODE START\n",
        "    words = [word for word in text.split() if word not in stop_words]\n",
        "    # YOUR CODE END\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99bd0b6b",
      "metadata": {
        "id": "99bd0b6b"
      },
      "source": [
        "## üß† –ó–∞–¥–∞–Ω–∏–µ 2: –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
        "–†–µ–∞–ª–∏–∑—É–π —Ñ—É–Ω–∫—Ü–∏—é `lemmatize_words`, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fa0b67df",
      "metadata": {
        "id": "fa0b67df"
      },
      "outputs": [],
      "source": [
        "def lemmatize_words(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # YOUR CODE STARTS HERE\n",
        "    words = text.split()\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "    # YOUR CODE ENDS HERE\n",
        "    return ' '.join(lemmatized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612fd9d5",
      "metadata": {
        "id": "612fd9d5"
      },
      "source": [
        "## üî¢ –ó–∞–¥–∞–Ω–∏–µ 3: TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n",
        "–†–µ–∞–ª–∏–∑—É–π —Ñ—É–Ω–∫—Ü–∏—é `vectorize_texts` –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –≤ TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "404246f6",
      "metadata": {
        "id": "404246f6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def vectorize_texts(texts):\n",
        "    # YOUR CODE STARTS HERE\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    # YOUR CODE ENDS HERE\n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50660b81",
      "metadata": {
        "id": "50660b81"
      },
      "source": [
        "## ü§ñ –ó–∞–¥–∞–Ω–∏–µ 4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
        "–î–æ–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é `train_and_predict`, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a580d5b2",
      "metadata": {
        "id": "a580d5b2"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def train_and_predict(texts, labels, sample):\n",
        "    # YOUR CODE STARTS HERE\n",
        "    model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "    model.fit(texts, labels)\n",
        "    # YOUR CODE ENDS HERE\n",
        "    return model.predict([sample])[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c9be63c",
      "metadata": {
        "id": "0c9be63c"
      },
      "source": [
        "## ‚úÖ –ê–≤—Ç–æ—Ç–µ—Å—Ç—ã –∫ –∑–∞–¥–∞–Ω–∏—è–º"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e27283ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e27283ea",
        "outputId": "74e9f1d0-0b06-46a5-e666-30a38f72d968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 3.645s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª: 10.0/10\n"
          ]
        }
      ],
      "source": [
        "import unittest\n",
        "\n",
        "class TestTextTasks(unittest.TestCase):\n",
        "    def test_clean(self):\n",
        "        global score\n",
        "        t = clean_text(\"<b>This is a Test!!!</b>\")\n",
        "        self.assertTrue(\"this\" not in t)\n",
        "        self.assertTrue(\"test\" in t)\n",
        "        score += 2.5\n",
        "    def test_lemmatize(self):\n",
        "        global score\n",
        "        l = lemmatize_words(\"cats running\")\n",
        "        self.assertIn(\"cat\", l)\n",
        "        self.assertIn(\"running\", l)  # Note: POS not provided\n",
        "        score += 2.5\n",
        "    def test_vector(self):\n",
        "        global score\n",
        "        X = vectorize_texts([\"dog barks\", \"cat meows\"])\n",
        "        self.assertEqual(X.shape[0], 2)\n",
        "        score += 2.5\n",
        "    def test_classifier(self):\n",
        "        global score\n",
        "        pred = train_and_predict([\"good movie\", \"bad product\"], [1, 0], \"great film\")\n",
        "        self.assertIn(pred, [0, 1])\n",
        "        score += 2.5\n",
        "\n",
        "global score\n",
        "score = 0\n",
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
        "max_score = 10\n",
        "print(f\"–ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª: {score}/{max_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY  !!!\n",
        "# DO NOT EXECUTE !!!\n",
        "# Save the result to Google Sheets\n",
        "from datetime import datetime\n",
        "\n",
        "# Get current date and time\n",
        "now = datetime.now()\n",
        "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "worksheet.append_row([student_name,group_id, task_id, score, timestamp])\n",
        "\n",
        "print(f\"Test completed! {student_name}, your score is {score}/{max_score}.\")"
      ],
      "metadata": {
        "id": "J3JcvoYvqp6g"
      },
      "id": "J3JcvoYvqp6g",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}